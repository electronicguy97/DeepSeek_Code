{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from accelerate import Accelerator\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\" # 메모리 조각 방지\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BASE_MODEL = \"./DeepSeek-R1-Distill-Llama-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"code_search_net\", \"python\")  # 언어 선택 가능 (ex: python, java)\n",
    "train_data = dataset[\"train\"]\n",
    "valid_data = dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    #target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\"],  # 가중치 적용할 레이어 빠른학습\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],  # 가중치 적용할 레이어 정확도\n",
    "    #target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"embed_tokens\", \"lm_head\"],  # 가중치 적용할 레이어 도메인 전체 학습\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# 4bit 양자화 설정 - QLoRA로 해야 함\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\", #nf4\n",
    "    bnb_4bit_use_double_quant=True, #True\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # 패딩 토큰 설정\n",
    "\n",
    "# 4-bit 양자화된 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    #device_map=\"sequential\",\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config  # 4-bit 설정 적용\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn 에러\n",
    "model.enable_input_require_grads() # get_input_embeddings().weight.requires_grad = True \n",
    "# LoRA 적용\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# LoRA가 적용된 레이어만 학습 (모델 파라미터 freeze)\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora\" in name:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # 함수 코드와 Docstring을 하나의 입력으로 결합\n",
    "    combined_texts = [\n",
    "        f\"{doc}\\n\\n{code}\" for doc, code in zip(examples[\"func_documentation_string\"], examples[\"func_code_string\"])\n",
    "    ]\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        combined_texts,  \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    tokenized[\"labels\"] = torch.tensor(tokenized[\"input_ids\"])  # ✅ `torch.tensor()` 사용\n",
    "    return tokenized\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./deepseek-code-doc-lora-V2\",\n",
    "    dataloader_pin_memory=True,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_checkpointing=False,  # T: GPU 메모리 절약\n",
    "    gradient_accumulation_steps=2,  # 작은 배치 크기 보완\n",
    "    num_train_epochs=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./code_logs\",\n",
    "    fp16=True,  # 16-bit 연산\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    ddp_find_unused_parameters=False  # DDP 활성화\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 가중치만 저장\n",
    "model.save_pretrained(\"./deepseek-code-doc-lora-V2\", safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"./deepseek-code-doc-lora-V2\")\n",
    "\n",
    "trainer.state.save_to_json(\"./deepseek-code-doc-lora-V2/trainer_state.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4bit 양자화 설정 - QLoRA로 해야 함\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\", #nf4\n",
    "    bnb_4bit_use_double_quant=True, #True\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # 패딩 토큰 설정\n",
    "\n",
    "# 4-bit 양자화된 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    #device_map=\"sequential\",\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config  # 4-bit 설정 적용\n",
    ")\n",
    "\n",
    "\n",
    "# LoRA 적용된 모델 불러오기\n",
    "lora_model = PeftModel.from_pretrained(model, \"./deepseek-code-doc-lora-V2\")\n",
    "\n",
    "# LoRA 병합 (LoRA 가중치를 원본 모델에 합침)\n",
    "merged_model = lora_model.merge_and_unload()\n",
    "\n",
    "# 병합된 모델 저장 (이제 일반 모델처럼 사용 가능)\n",
    "merged_model.save_pretrained(\"./deepseek-code-doc-merged-V2\")\n",
    "tokenizer.save_pretrained(\"./deepseek-code-doc-merged-V2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(mixed_precision=\"fp16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4bit 양자화 설정 - QLoRA로 해야 함\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\", #nf4\n",
    "    bnb_4bit_use_double_quant=True, #True\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "# 기본 모델 로드 (LoRA 적용 전 원본 모델)\n",
    "lora_model = AutoModelForCausalLM.from_pretrained(\"./deepseek-code-doc-merged-V2\", device_map='auto', quantization_config=bnb_config)\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./deepseek-code-doc-merged-V2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accelerator 준비 (mixed precision 적용)\n",
    "lora_model = accelerator.prepare(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 생성 파이프라인 (dispatch_model 없이 바로 사용)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=lora_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,  # ✅ 토큰 수 절약\n",
    "    device_map='auto',   # ✅ 여러 GPU 자동 분배\n",
    "    #offload_folder=\"./offload\",  # CPU로 오프로드하여 GPU 메모리 절약\n",
    "    #offload_state_dict=True  # 상태 딕셔너리 오프로드\n",
    ")\n",
    "\n",
    "# 프롬프트 설정\n",
    "document = f\"\"\"\n",
    "1 + 1 코드를 만들어 주세요\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"\"\"\n",
    "    1. 코드보여줘\n",
    "    2. 반드시 한국어로 설명해\n",
    "    Document:\n",
    "    {document}\n",
    "    \"\"\"},\n",
    "]\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# 생성\n",
    "with torch.no_grad():\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "        do_sample=True,\n",
    "        temperature=0.5,  # 샘플링의 다양성을 높이기 위한 온도 설정\n",
    "        top_k=40,  # 가장 높은 확률을 가진 top k 개의 단어만 고려\n",
    "        top_p=0.8,  # 누적 확률이 80%인 단어들만 고려\n",
    "        repetition_penalty=1.2,\n",
    "        add_special_tokens=True,\n",
    "        eos_token_id=[  \n",
    "            pipe.tokenizer.eos_token_id,\n",
    "            pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# 결과 출력\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EOS Token ID:\", tokenizer.eos_token_id)\n",
    "print(\"모든 특수 토큰:\", tokenizer.all_special_tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
